{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset of Network Analysis for ABCD data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://dx.plos.org/10.1371/journal.pbio.1002328  \n",
    "https://www.sciencedirect.com/science/article/pii/S105381191730109X?via%3Dihub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gracer/.local/lib/python3.6/site-packages/vispy/visuals/isocurve.py:22: UserWarning: VisPy is not yet compatible with matplotlib 2.2+\n",
      "  warnings.warn(\"VisPy is not yet compatible with matplotlib 2.2+\")\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import community\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "import pickle\n",
    "import pdb\n",
    "import statistics\n",
    "import matplotlib\n",
    "matplotlib.use(\"Qt5Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from visbrain.objects import ConnectObj, SceneObj, SourceObj, BrainObj\n",
    "from visbrain.io import download_file\n",
    "\n",
    "import bct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('/Users/gracer/Google Drive/ABCD/important_txt/locations.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('/Users/gracer/Google Drive/ABCD/tmp/graphAna4', 'rb') as pickle_file:\n",
    "    try:\n",
    "        while True:\n",
    "            GRAPHS = pickle.load(pickle_file)\n",
    "            print (GRAPHS)\n",
    "    except EOFError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['edges', 'correlations', 'mean_FC', 'graphs', 'BIGdf'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GRAPHS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIGdf=GRAPHS['BIGdf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making dataframe to use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PID = list(GRAPHS['mean_FC'].keys())\n",
    "len(PID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex=[]\n",
    "PCS=[]\n",
    "OVOB=[]\n",
    "mean_FC=[]\n",
    "sd_FC=[]\n",
    "for item in list(GRAPHS['mean_FC'].values()):\n",
    "    sex.append(item[-5])\n",
    "    PCS.append(item[-4])\n",
    "    OVOB.append(item[-3])\n",
    "    mean_FC.append(item[-2])\n",
    "    sd_FC.append(item[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(OVOB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FCmat=np.column_stack([PID, sex, PCS, OVOB, mean_FC, sd_FC])\n",
    "FCdf=pd.DataFrame(FCmat, columns = ['subjects','sex','PCS','OVOB','mean','SD'])\n",
    "FCdf=FCdf.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subjects</th>\n",
       "      <th>sex</th>\n",
       "      <th>PCS</th>\n",
       "      <th>OVOB</th>\n",
       "      <th>mean</th>\n",
       "      <th>SD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>120</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>120</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>sub-NDARINVFU4GY5DE</td>\n",
       "      <td>M</td>\n",
       "      <td>latepubertal</td>\n",
       "      <td>Normalweight</td>\n",
       "      <td>0.33033018133034897</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   subjects  sex           PCS          OVOB  \\\n",
       "count                   120  120           120           120   \n",
       "unique                  120    2             3             3   \n",
       "top     sub-NDARINVFU4GY5DE    M  latepubertal  Normalweight   \n",
       "freq                      1   63            44            44   \n",
       "\n",
       "                       mean   SD  \n",
       "count                   120  120  \n",
       "unique                  120  118  \n",
       "top     0.33033018133034897  NaN  \n",
       "freq                      1    3  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FCdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subjects</th>\n",
       "      <th>sex</th>\n",
       "      <th>PCS</th>\n",
       "      <th>OVOB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sub-NDARINVULZMADF1</td>\n",
       "      <td>F</td>\n",
       "      <td>latepubertal</td>\n",
       "      <td>Obese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sub-NDARINVULZMADF1</td>\n",
       "      <td>F</td>\n",
       "      <td>latepubertal</td>\n",
       "      <td>Obese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sub-NDARINVULZMADF1</td>\n",
       "      <td>F</td>\n",
       "      <td>latepubertal</td>\n",
       "      <td>Obese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sub-NDARINVULZMADF1</td>\n",
       "      <td>F</td>\n",
       "      <td>latepubertal</td>\n",
       "      <td>Obese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sub-NDARINVD9L81NY5</td>\n",
       "      <td>F</td>\n",
       "      <td>latepubertal</td>\n",
       "      <td>Obese</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              subjects sex           PCS   OVOB\n",
       "0  sub-NDARINVULZMADF1   F  latepubertal  Obese\n",
       "1  sub-NDARINVULZMADF1   F  latepubertal  Obese\n",
       "2  sub-NDARINVULZMADF1   F  latepubertal  Obese\n",
       "3  sub-NDARINVULZMADF1   F  latepubertal  Obese\n",
       "4  sub-NDARINVD9L81NY5   F  latepubertal  Obese"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = BIGdf[['subjects', 'sex','PCS','OVOB']]\n",
    "categories.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Create a Mega graph by OVOB and PCS groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_total_graphs(dict_o_data):\n",
    "    mylist=[]\n",
    "    for key, val_list in dict_o_data.items():\n",
    "        for i in val_list:\n",
    "            cor_matrix = np.asarray(i)\n",
    "            mylist.append(cor_matrix)\n",
    "    x=np.stack(mylist, axis=2)\n",
    "    mu=np.mean(x, axis=(2))\n",
    "    return(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['edges', 'correlations', 'mean_FC', 'graphs', 'BIGdf'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GRAPHS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[matrix([[0.      , 0.297177, 0.366261, ..., 0.712325, 0.027972, 0.032292],\n",
       "         [0.297177, 0.      , 1.067256, ..., 0.267867, 0.482737, 0.50939 ],\n",
       "         [0.366261, 1.067256, 0.      , ..., 0.583668, 0.510977, 0.520726],\n",
       "         ...,\n",
       "         [0.712325, 0.267867, 0.583668, ..., 0.      , 0.253103, 0.211843],\n",
       "         [0.027972, 0.482737, 0.510977, ..., 0.253103, 0.      , 1.947065],\n",
       "         [0.032292, 0.50939 , 0.520726, ..., 0.211843, 1.947065, 0.      ]]),\n",
       " matrix([[ 0.      ,  0.618682,  0.288411, ...,  0.212559,  0.336378,\n",
       "           0.047735],\n",
       "         [ 0.618682,  0.      ,  0.972095, ...,  0.283186,  0.510952,\n",
       "           0.318188],\n",
       "         [ 0.288411,  0.972095, 18.714974, ...,  0.419613,  0.701422,\n",
       "           0.603747],\n",
       "         ...,\n",
       "         [ 0.212559,  0.283186,  0.419613, ...,  0.      ,  0.383667,\n",
       "           0.340115],\n",
       "         [ 0.336378,  0.510952,  0.701422, ...,  0.383667,  0.      ,\n",
       "           1.268736],\n",
       "         [ 0.047735,  0.318188,  0.603747, ...,  0.340115,  1.268736,\n",
       "           0.      ]])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(GRAPHS['correlations'].values())[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BMI and PCS GRAPHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "no_subset=categories.loc[(categories['OVOB'] == 'Normalweight')]\n",
    "no_dict = {k: GRAPHS['correlations'][k] for k in no_subset['subjects'] if k in GRAPHS['correlations']}\n",
    "\n",
    "ov_subset=categories.loc[(categories['OVOB'] == 'Overweight')]\n",
    "ov_dict = {k: GRAPHS['correlations'][k] for k in ov_subset['subjects'] if k in GRAPHS['correlations']}\n",
    "\n",
    "ob_subset=categories.loc[(categories['OVOB'] == 'Obese')]\n",
    "ob_dict = {k: GRAPHS['correlations'][k] for k in ob_subset['subjects'] if k in GRAPHS['correlations']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "early_subset=categories.loc[(categories['PCS'] == 'earlypubertal')]\n",
    "early_dict = {k: GRAPHS['correlations'][k] for k in early_subset['subjects'] if k in GRAPHS['correlations']}\n",
    "\n",
    "late_subset=categories.loc[(categories['PCS'] == 'latepubertal')]\n",
    "late_dict = {k: GRAPHS['correlations'][k] for k in late_subset['subjects'] if k in GRAPHS['correlations']}\n",
    "\n",
    "mid_subset=categories.loc[(categories['PCS'] == 'midpubertal')]\n",
    "mid_dict = {k: GRAPHS['correlations'][k] for k in mid_subset['subjects'] if k in GRAPHS['correlations']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_no=make_total_graphs(no_dict)\n",
    "mean_ov=make_total_graphs(ov_dict)\n",
    "mean_ob=make_total_graphs(ob_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_early=make_total_graphs(early_dict)\n",
    "mean_late=make_total_graphs(late_dict)\n",
    "mean_mid=make_total_graphs(mid_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creates graph using the data of the correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "noG = nx.from_numpy_matrix(mean_no)\n",
    "ovG = nx.from_numpy_matrix(mean_ov)\n",
    "obG = nx.from_numpy_matrix(mean_ob)\n",
    "OVOBgraphs = [noG, ovG, obG]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyG = nx.from_numpy_matrix(mean_early)\n",
    "midG = nx.from_numpy_matrix(mean_mid)\n",
    "lateG = nx.from_numpy_matrix(mean_late)\n",
    "PCSgraphs = [earlyG, midG, lateG]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for graph in OVOBgraphs:\n",
    "    for i, nlrow in labels.iterrows():\n",
    "        graph.node[i].update(nlrow[0:].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for graph in PCSgraphs:\n",
    "    for i, nlrow in labels.iterrows():\n",
    "        graph.node[i].update(nlrow[0:].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_edges,no_weights = zip(*nx.get_edge_attributes(noG,'weight').items())\n",
    "ov_edges,ov_weights = zip(*nx.get_edge_attributes(ovG,'weight').items())\n",
    "ob_edges,ob_weights = zip(*nx.get_edge_attributes(obG,'weight').items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_edges, early_weights = zip(*nx.get_edge_attributes(earlyG,'weight').items())\n",
    "mid_edges,mid_weights = zip(*nx.get_edge_attributes(midG,'weight').items())\n",
    "late_edges,late_weights = zip(*nx.get_edge_attributes(lateG,'weight').items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for graph in OVOBgraphs:\n",
    "    d = nx.degree(graph)\n",
    "    list(d)[0]\n",
    "    nodelist, node_sizes = zip(*list(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for graph in PCSgraphs:\n",
    "    d = nx.degree(graph)\n",
    "    list(d)[0]\n",
    "    nodelist, node_sizes = zip(*list(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corr_network(G, corr_direction, min_correlation):\n",
    "    ##Creates a copy of the graph\n",
    "    H = G.copy()\n",
    "    \n",
    "    ##Checks all the edges and removes some based on corr_direction\n",
    "    for stock1, stock2, weight in list(G.edges(data=True)):\n",
    "        ##if we only want to see the positive correlations we then delete the edges with weight smaller than 0        \n",
    "        if corr_direction == \"positive\":\n",
    "            ####it adds a minimum value for correlation. \n",
    "            ####If correlation weaker than the min, then it deletes the edge\n",
    "            if weight[\"weight\"] <0 or weight[\"weight\"] < min_correlation:\n",
    "                H.remove_edge(stock1, stock2)\n",
    "        ##this part runs if the corr_direction is negative and removes edges with weights equal or largen than 0\n",
    "        else:\n",
    "            ####it adds a minimum value for correlation. \n",
    "            ####If correlation weaker than the min, then it deletes the edge\n",
    "            if weight[\"weight\"] >=0 or weight[\"weight\"] > min_correlation:\n",
    "                H.remove_edge(stock1, stock2)\n",
    "                \n",
    "    \n",
    "    #crates a list for edges and for the weights\n",
    "    edges,weights = zip(*nx.get_edge_attributes(H,'weight').items())\n",
    "    \n",
    "    ### increases the value of weights, so that they are more visible in the graph\n",
    "    weights = tuple([(1+abs(x))**1 for x in weights])\n",
    "    \n",
    "    #####calculates the degree of each node\n",
    "    d = nx.degree(H)\n",
    "    #####creates list of nodes and a list their degrees that will be used later for their sizes\n",
    "    nodelist, node_sizes = zip(*list(d))\n",
    "    return(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corr_network_1(G):\n",
    "    #crates a list for edges and for the weights\n",
    "    edges,weights = zip(*nx.get_edge_attributes(G,'weight').items())\n",
    "\n",
    "    #positions\n",
    "    positions=nx.circular_layout(G)\n",
    "    \n",
    "    #Figure size\n",
    "    plt.figure(figsize=(15,15))\n",
    "\n",
    "    #draws nodes\n",
    "    nx.draw_networkx_nodes(G,positions,node_color='#DA70D6',\n",
    "                           node_size=500,alpha=0.8)\n",
    "    \n",
    "    #Styling for labels\n",
    "    nx.draw_networkx_labels(G, positions, font_size=8, \n",
    "                            font_family='sans-serif')\n",
    "        \n",
    "    #draws the edges\n",
    "    nx.draw_networkx_edges(G, positions, edge_list=edges,style='solid')\n",
    "    \n",
    "    # displays the graph without axis\n",
    "    plt.axis('off')\n",
    "    #saves image\n",
    "    plt.savefig(\"part1.png\", format=\"PNG\")\n",
    "    plt.show() \n",
    "\n",
    "#create_corr_network_1(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_corr_network' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bb8a68ca112d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnoH\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_corr_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"positive\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0movH\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_corr_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0movG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"positive\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mobH\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_corr_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"positive\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msaveme\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msaveme\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'noH'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_corr_network' is not defined"
     ]
    }
   ],
   "source": [
    "noH=create_corr_network(noG, \"positive\",0)\n",
    "ovH=create_corr_network(ovG, \"positive\", 0)\n",
    "obH=create_corr_network(obG, \"positive\", 0)\n",
    "saveme={}\n",
    "saveme['noH']=noH\n",
    "saveme['ovH']=ovH\n",
    "saveme['obH']=obH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyH=create_corr_network(earlyG, \"positive\",0)\n",
    "midH=create_corr_network(midG, \"positive\", 0)\n",
    "lateH=create_corr_network(lateG, \"positive\", 0)\n",
    "saveme={}\n",
    "saveme['earlyH']=earlyH\n",
    "saveme['midH']=midH\n",
    "saveme['lateH']=lateH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting partition values with the large graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "noPART = community.best_partition(noH)\n",
    "ovPART = community.best_partition(ovH)\n",
    "obPART = community.best_partition(obH)\n",
    "\n",
    "# noPART.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyPART = community.best_partition(earlyH)\n",
    "midPART = community.best_partition(midH)\n",
    "latePART = community.best_partition(lateH)\n",
    "\n",
    "# noPART.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "noPARTvalues = np.asarray(list(noPART.values()))\n",
    "obPARTvalues = np.asarray(list(obPART.values()))\n",
    "ovPARTvalues = np.asarray(list(ovPART.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyPARTvalues = np.asarray(list(earlyPART.values()))\n",
    "latePARTvalues = np.asarray(list(latePART.values()))\n",
    "midPARTvalues = np.asarray(list(midPART.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the normalized mutual disribution information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_no_ov=normalized_mutual_info_score(list(noPART.values()), list(ovPART.values()))\n",
    "print(diff_no_ov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_no_ob=normalized_mutual_info_score(list(noPART.values()), list(obPART.values()))\n",
    "print(diff_no_ob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_early_mid=normalized_mutual_info_score(list(earlyPART.values()), list(midPART.values()))\n",
    "print(diff_early_mid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_early_late=normalized_mutual_info_score(list(earlyPART.values()), list(latePART.values()))\n",
    "print(diff_early_late)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary so far\n",
    "Larger difference between the normal weight and obese compared to the normal weight and the overweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dic = {0:'blue',1:'red',2:'green',3:'purple',4:'yellow',5:'pink',6:'black',7:'orange', 8:'cyan'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PARTS=[noPART, ovPART ,obPART]\n",
    "for item in PARTS:\n",
    "    for key, value in item.items():\n",
    "        item[key]=(color_dic[value])\n",
    "#     print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in noPART.items():\n",
    "    noH.node(data=True)[key].update({'color':value})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drawing\n",
    "size = float(len(set(ovPART.values())))\n",
    "print(size)\n",
    "pos = nx.spring_layout(ovH)\n",
    "count = 0\n",
    "for com in set(ovPART.values()) :\n",
    "    count = count + 1.\n",
    "    list_nodes = [nodes for nodes in ovPART.keys()\n",
    "                                if ovPART[nodes] == com]\n",
    "    nx.draw_networkx_nodes(ovH, pos, list_nodes, node_size = 20,\n",
    "                                node_color = str(count / size))\n",
    "\n",
    "\n",
    "nx.draw_networkx_edges(ovH, pos, alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in ovPART.items():\n",
    "    ovH.node(data=True)[key].update({'color':value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in obPART.items():\n",
    "    obH.node(data=True)[key].update({'color':value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(H.edges(data=True))[0]\n",
    "# H.edges(data=True)\n",
    "no_edge_weights = [e[2]['weight'] for e in noH.edges(data=True)]\n",
    "ov_edge_weights = [e[2]['weight'] for e in ovH.edges(data=True)]\n",
    "ob_edge_weights = [e[2]['weight'] for e in obH.edges(data=True)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define node positions data structure (dict) for plotting\n",
    "no_node_colors = {node[0]: (node[1]['color']) for node in noH.nodes(data=True)}\n",
    "ov_node_colors = {node[0]: (node[1]['color']) for node in ovH.nodes(data=True)}\n",
    "ob_node_colors = {node[0]: (node[1]['color']) for node in obH.nodes(data=True)}\n",
    "\n",
    "# Preview of node_positions with a bit of hack (there is no head/slice method for dictionaries).\n",
    "#dict(list(node_colors.items())[0:5])\n",
    "#print(G.node(data=True))\n",
    "no_list_colors=list(no_node_colors.values())\n",
    "ov_list_colors=list(ov_node_colors.values())\n",
    "ob_list_colors=list(ob_node_colors.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_node_positions = {node[0]: (node[1]['X'], -node[1]['Y']) for node in noH.nodes(data=True)}\n",
    "ov_node_positions = {node[0]: (node[1]['X'], -node[1]['Y']) for node in ovH.nodes(data=True)}\n",
    "ob_node_positions = {node[0]: (node[1]['X'], -node[1]['Y']) for node in obH.nodes(data=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define node positions data structure (dict) for plotting\n",
    "# node_positions = {node[0]: (node[1]['X'], -node[1]['Y']) for node in H.nodes(data=True)}\n",
    "# node_colors = {node[0]: (node[1]['color']) for node in H.nodes(data=True)}\n",
    "\n",
    "# Preview of node_positions with a bit of hack (there is no head/slice method for dictionaries).\n",
    "#dict(list(node_colors.items())[0:5])\n",
    "#print(G.node(data=True))\n",
    "# list_colors=list(node_colors.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PARTS=[earlyPART, midPART ,latePART]\n",
    "for item in PARTS:\n",
    "    for key, value in item.items():\n",
    "        item[key]=(color_dic[value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in earlyPART.items():\n",
    "    earlyH.node(data=True)[key].update({'color':value})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "blue=[]\n",
    "for node in noH.nodes(data=True):\n",
    "    if node[1]['color']=='blue':\n",
    "#         print(node[0])\n",
    "        blue.append(node[0])\n",
    "#         pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "blue_ob=[]\n",
    "for node in obH.nodes(data=True):\n",
    "    if node[1]['color']=='blue':\n",
    "#         print(node[0])\n",
    "        blue_ob.append(node[0])\n",
    "#         pdb.set_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE\n",
    "G = nx.karate_club_graph()\n",
    "res = [0,1,2,3,4,5, 'parrot'] #I've added 'parrot', a node that's not in G\n",
    "                              #just to demonstrate that G.subgraph is okay\n",
    "                              #with nodes not in G.    \n",
    "pos = nx.spring_layout(G)  #setting the positions with respect to G, not k.\n",
    "k = G.subgraph(res)  \n",
    "\n",
    "plt.figure()\n",
    "nx.draw_networkx(k, pos=pos)\n",
    "\n",
    "othersubgraph = G.subgraph(range(6,G.order()))\n",
    "nx.draw_networkx(othersubgraph, pos=pos, node_color = 'b')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G = nx.karate_club_graph()\n",
    "# res = [0,1,2,3,4,5, 'parrot'] #I've added 'parrot', a node that's not in G\n",
    "                              #just to demonstrate that G.subgraph is okay\n",
    "                              #with nodes not in G.    \n",
    "pos = nx.spring_layout(noG)  #setting the positions with respect to G, not k.\n",
    "subNo = noG.subgraph(blue)  \n",
    "\n",
    "plt.figure()\n",
    "nx.draw_networkx(subNo, pos=pos, node_color='b')\n",
    "\n",
    "# othersubgraph = noG.subgraph(range(50,noG.order()))\n",
    "# nx.draw_networkx(othersubgraph, pos=pos, node_color = 'g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G = nx.karate_club_graph()\n",
    "# res = [0,1,2,3,4,5, 'parrot'] #I've added 'parrot', a node that's not in G\n",
    "                              #just to demonstrate that G.subgraph is okay\n",
    "                              #with nodes not in G.    \n",
    "pos = nx.spring_layout(obG)  #setting the positions with respect to G, not k.\n",
    "subOB = obG.subgraph(blue_ob)  \n",
    "\n",
    "plt.figure()\n",
    "nx.draw_networkx(subOB, pos=pos, node_color='b')\n",
    "\n",
    "# othersubgraph = noG.subgraph(range(50,noG.order()))\n",
    "# nx.draw_networkx(othersubgraph, pos=pos, node_color = 'g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NOmat=nx.to_numpy_array(G=noG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "OBmat=nx.to_numpy_array(G=obG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVmat=nx.to_numpy_array(G=ovG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EARLYmat=nx.to_numpy_array(G=earlyG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIDmat=nx.to_numpy_array(G=midG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATEmat=nx.to_numpy_array(G=lateG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bctpy --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Participation Coefficient\n",
    "Partition networks into the modules, calculate the PC per node within each group. Higher PC indicates more distributed between network connectivity, while a PC of 0 signifies a nodeâ€™s links are completely within its home network (within network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PCno = bct.participation_coef(W=NOmat, ci= noPARTvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCob = bct.participation_coef(W=OBmat, ci= obPARTvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCov = bct.participation_coef(W=OVmat, ci= ovPARTvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCearly = bct.participation_coef(W=EARLYmat, ci= earlyPARTvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCmid = bct.participation_coef(W=MIDmat, ci= midPARTvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PClate = bct.participation_coef(W=LATEmat, ci= latePARTvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alls = np.vstack([PCno, PCov, PCob, PCearly, PCmid, PClate])\n",
    "alls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Alls = pd.DataFrame(alls)\n",
    "Alls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alls.to_csv(\"/Users/gracer/Google Drive/ABCD/tmp/PC.csv\", sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_edge_weights = [e[2]['weight'] for e in earlyH.edges(data=True)]\n",
    "mid_edge_weights = [e[2]['weight'] for e in midH.edges(data=True)]\n",
    "late_edge_weights = [e[2]['weight'] for e in lateH.edges(data=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_edge_keep = []\n",
    "for e in earlyH.edges(data=True):\n",
    "    if e[2]['weight']>=1:\n",
    "        early_edge_keep.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#drawing\n",
    "size = float(len(set(earlyPART.values())))\n",
    "print(size)\n",
    "pos = nx.spring_layout(earlyH)\n",
    "count = 0.\n",
    "for com in set(earlyPART.values()) :\n",
    "    print(com)\n",
    "    count = count + 1.\n",
    "    list_nodes = [nodes for nodes in earlyPART.keys()\n",
    "                                if earlyPART[nodes] == com]\n",
    "    nx.draw_networkx_nodes(earlyH, pos, list_nodes, node_size = 20,edge_vmin = 0.5, \n",
    "                                node_color = no_list_colors)\n",
    "\n",
    "\n",
    "nx.draw_networkx_edges(earlyH, pos, alpha=0.5, edgelist=early_edge_keep)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in midPART.items():\n",
    "    midH.node(data=True)[key].update({'color':value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in latePART.items():\n",
    "    lateH.node(data=True)[key].update({'color':value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define node positions data structure (dict) for plotting\n",
    "early_node_colors = {node[0]: (node[1]['color']) for node in earlyH.nodes(data=True)}\n",
    "mid_node_colors = {node[0]: (node[1]['color']) for node in midH.nodes(data=True)}\n",
    "late_node_colors = {node[0]: (node[1]['color']) for node in lateH.nodes(data=True)}\n",
    "\n",
    "# Preview of node_positions with a bit of hack (there is no head/slice method for dictionaries).\n",
    "#dict(list(node_colors.items())[0:5])\n",
    "#print(G.node(data=True))\n",
    "early_list_colors=list(early_node_colors.values())\n",
    "mid_list_colors=list(mid_node_colors.values())\n",
    "late_list_colors=list(late_node_colors.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_node_positions = {node[0]: (node[1]['X'], -node[1]['Y']) for node in earlyH.nodes(data=True)}\n",
    "mid_node_positions = {node[0]: (node[1]['X'], -node[1]['Y']) for node in midH.nodes(data=True)}\n",
    "late_node_positions = {node[0]: (node[1]['X'], -node[1]['Y']) for node in lateH.nodes(data=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define node positions data structure (dict) for plotting\n",
    "# node_positions = {node[0]: (node[1]['X'], -node[1]['Y']) for node in H.nodes(data=True)}\n",
    "# node_colors = {node[0]: (node[1]['color']) for node in H.nodes(data=True)}\n",
    "\n",
    "# # Preview of node_positions with a bit of hack (there is no head/slice method for dictionaries).\n",
    "# #dict(list(node_colors.items())[0:5])\n",
    "# #print(G.node(data=True))\n",
    "# list_colors=list(node_colors.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "nx.draw(noH, pos=no_node_positions,  node_size=20,  edge_color = \"lightgrey\", node_color = no_list_colors)\n",
    "plt.title('normal', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "nx.draw(earlyH, pos=early_node_positions,  node_size=20,  edge_color = \"lightgrey\", node_color = early_list_colors)\n",
    "plt.title('early', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "nx.draw(obH, pos=ob_node_positions,  node_size=20,  edge_color = \"lightgrey\", node_color = ob_list_colors)\n",
    "plt.title('obese', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "nx.draw(midH, pos=mid_node_positions,  node_size=20,  edge_color = \"lightgrey\", node_color = mid_list_colors)\n",
    "plt.title('Mid', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "nx.draw(lateH, pos=late_node_positions,  node_size=20,  edge_color = \"lightgrey\", node_color = late_list_colors)\n",
    "plt.title('Late', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['edges', 'correlations', 'mean_FC', 'graphs', 'BIGdf'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GRAPHS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gracer/anaconda3/lib/python3.7/site-packages/networkx/drawing/nx_pylab.py:611: MatplotlibDeprecationWarning: isinstance(..., numbers.Number)\n",
      "  if cb.is_numlike(alpha):\n"
     ]
    }
   ],
   "source": [
    "nog=community.induced_graph(noPART,noH)\n",
    "create_corr_network_1(G=nog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovg=community.induced_graph(ovPART,ovH)\n",
    "create_corr_network_1(G=ovg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obg=community.induced_graph(obPART,obH)\n",
    "create_corr_network_1(G=obg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyg=community.induced_graph(earlyPART,earlyH)\n",
    "create_corr_network_1(G=earlyg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midg=community.induced_graph(midPART,midH)\n",
    "create_corr_network_1(G=midg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lateg=community.induced_graph(latePART,lateH)\n",
    "create_corr_network_1(G=lateg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summmary of results so far... \n",
    "Looks like the obese have one less module compared to the overweight and the normal weight \n",
    "Need to assess what each node is comprised of  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define node positions data structure (dict) for plotting\n",
    "no_node_positions = {node[0]: (node[1]['X'], -node[1]['Y'], node[1]['Z']) for node in noH.nodes(data=True)}\n",
    "no_node_colors = {node[0]: (node[1]['color']) for node in noH.nodes(data=True)}\n",
    "\n",
    "# Preview of node_positions with a bit of hack (there is no head/slice method for dictionaries).\n",
    "#dict(list(node_colors.items())[0:5])\n",
    "#print(G.node(data=True))\n",
    "list_colors=list(node_colors.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download data\n",
    "# arch = np.load(download_file('phase_sync_delta.npz', astype='example_data'))\n",
    "nodes, edges = arch['nodes'], arch['edges']\n",
    "# Create the scene with a black background\n",
    "sc = SceneObj(size=(1500, 600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(arch['nodes'].shape)\n",
    "print(arch['edges'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "# arch = np.load(download_file('phase_sync_delta.npz', astype='example_data'))\n",
    "nodes, edges = np.asarray(list(no_node_positions.values())), mean_no\n",
    "# Create the scene with a black background\n",
    "sc = SceneObj(size=(1500, 600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coloring method\n",
    "color_by = 'strength'\n",
    "# Because we don't want to plot every connections, we only keep connections\n",
    "# above .7\n",
    "select = edges > .5\n",
    "# Define the connectivity object\n",
    "c_default = ConnectObj('default', nodes, edges, select=select, line_width=2.,\n",
    "                       cmap='Spectral_r', color_by=color_by)\n",
    "# Then, we define the sources\n",
    "s_obj = SourceObj('sources', nodes, color='#ab4642', radius_min=15.)\n",
    "sc.add_to_subplot(c_default, title='Color by connectivity strength')\n",
    "# And add connect, source and brain objects to the scene\n",
    "sc.add_to_subplot(s_obj)\n",
    "sc.add_to_subplot(BrainObj('B3'), use_this_cam=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coloring method\n",
    "color_by = 'count'\n",
    "# Weak connections -> alpha = .1 // strong connections -> alpha = 1.\n",
    "dynamic = (.1, 1.)\n",
    "# Define the connectivity and source object\n",
    "c_count = ConnectObj('default', nodes, edges, select=select, line_width=4.,\n",
    "                     color_by=color_by, antialias=True, dynamic=dynamic)\n",
    "s_obj_c = SourceObj('sources', nodes, color='olive', radius_min=10.,\n",
    "                    symbol='square')\n",
    "# And add connect, source and brain objects to the scene\n",
    "sc.add_to_subplot(c_count, row=0, col=1,\n",
    "                  title='Color by number of connections per node')\n",
    "sc.add_to_subplot(s_obj_c, use_this_cam=True, row=0, col=1)\n",
    "sc.add_to_subplot(BrainObj('B3'), use_this_cam=True, row=0, col=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we take a copy of the connectivity array\n",
    "edges_copy = edges.copy()\n",
    "# Then, we force edges to take fixed values\n",
    "# ====================   =========  ===========\n",
    "# Condition              New value  Color\n",
    "# ====================   =========  ===========\n",
    "# edges >= 0.8              4.      red\n",
    "# edges in [.78, .8[        3.      orange\n",
    "# edges in [.74, .78[       2.      blue\n",
    "# Others                    -       lightgray\n",
    "# ====================   =========  ===========\n",
    "edges_copy[edges_copy >= .8] = 4.\n",
    "edges_copy[np.logical_and(edges_copy >= .78, edges_copy < .8)] = 3.\n",
    "edges_copy[np.logical_and(edges_copy >= .74, edges_copy < .78)] = 2.\n",
    "# Now we use a dctionary to set one color per value.\n",
    "ccol = {\n",
    "    None: 'lightgray',\n",
    "    2.: 'blue',\n",
    "    3.: 'orange',\n",
    "    4.: 'red'\n",
    "}\n",
    "\n",
    "# Define the connectivity and source objects\n",
    "c_cuscol = ConnectObj('default', nodes, edges_copy, select=edges > .7,\n",
    "                      custom_colors=ccol)\n",
    "s_obj_cu = SourceObj('sources', nodes, color='slategray', radius_min=10.,\n",
    "                     symbol='ring')\n",
    "# Add objects to the scene\n",
    "sc.add_to_subplot(c_cuscol, row=0, col=2, title='Custom colors')\n",
    "sc.add_to_subplot(s_obj_cu, row=0, col=2)\n",
    "sc.add_to_subplot(BrainObj('white'), use_this_cam=True, row=0, col=2)\n",
    "\n",
    "# Finally, display the scene\n",
    "sc.preview()\n",
    "sc.screenshot('/Users/gracer/Google Drive/ABCD/tmp/figlayout.png', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_ROIs = list(noPART.keys())\n",
    "no_MODS = list(noPART.values())\n",
    "\n",
    "ov_ROIs = list(ovPART.keys())\n",
    "ov_MODS = list(ovPART.values())\n",
    "\n",
    "ob_ROIs = list(obPART.keys())\n",
    "ob_MODS = list(obPART.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nodf = pd.DataFrame(no_MODS)\n",
    "nodf['ROI'] = no_ROIs\n",
    "# nodf.to_csv(\"/Users/gracer/Google Drive/ABCD/tmp/normal_Module.csv\", sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ovdf = pd.DataFrame(ov_MODS)\n",
    "ovdf['ROI'] = ov_ROIs\n",
    "# ovdf.to_csv(\"/Users/gracer/Google Drive/ABCD/tmp/overweight_Module.csv\", sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "obdf = pd.DataFrame(ob_MODS)\n",
    "obdf['ROI'] = ob_ROIs\n",
    "# obdf.to_csv(\"/Users/gracer/Google Drive/ABCD/tmp/obese_Module.csv\", sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_ROIs = list(earlyPART.keys())\n",
    "early_MODS = list(earlyPART.values())\n",
    "\n",
    "mid_ROIs = list(midPART.keys())\n",
    "mid_MODS = list(midPART.values())\n",
    "\n",
    "late_ROIs = list(latePART.keys())\n",
    "late_MODS = list(latePART.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "earlydf = pd.DataFrame(early_MODS)\n",
    "earlydf['ROI'] = early_ROIs\n",
    "earlydf.to_csv(\"/Users/gracer/Google Drive/ABCD/tmp/early_Module.csv\", sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "middf = pd.DataFrame(mid_MODS)\n",
    "middf['ROI'] = mid_ROIs\n",
    "middf.to_csv(\"/Users/gracer/Google Drive/ABCD/tmp/mid_Module.csv\", sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "latedf = pd.DataFrame(late_MODS)\n",
    "latedf['ROI'] = late_ROIs\n",
    "latedf.to_csv(\"/Users/gracer/Google Drive/ABCD/tmp/late_Module.csv\", sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parcelation \n",
    "Through BIAC https://wiki.biac.duke.edu/biac:analysis:resting_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual and Group Matrices\n",
    "Network-level analysis will be performed with inividual correltion matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thresholding\n",
    "In accordance with van den Heuvel et al. 2017, we will examine and test statistical differences in functional connectivity (FC) defined as the mean of the correlation matrix. FC will be included in statistical tests between groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning\n",
    "Will partition full 264 connectome into modules using louvain algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the partition\n",
    "Will use normalized mutual information to assess similarity between network assignments. NMI measures information shared between two probability distribution functions, specifically measuring how much knowing one distribution leads to certainty ofthe other. Permuted the labels of individual matrices between contrasts 1,000 times to generate a null distribution of NMI values for each contrast. Matrices between groups were randomly shuffled and partitioned into functional networks, and NMI was calculated.   \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.normalized_mutual_info_score.html#sklearn.metrics.normalized_mutual_info_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connectivity Strength\n",
    "Caluclate Euclidean distance for each ROI-ROI pair. Linear regression with distance as a predictor of connectivity strength between groups.  \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "X = [[0, 1], [1, 1]]\n",
    "print(X)\n",
    "# distance between rows of X\n",
    "euclidean_distances(X, X)\n",
    "\n",
    "# get distance to origin\n",
    "# euclidean_distances(X, [[0, 0]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Within network changes\n",
    "All within network pairwise relationships were averaged per group. Two-tailed T-test to assess differences. Bonferroni corrections as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Between network changes\n",
    "Average connectivity is calculated per network. Compare the between network interactions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPHS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unthresh ={\"noG\":noG, \"ovG\":ovG, \"obG\":obG, \"earlyG\":earlyG, \"midG\":midG, \"lateG\":lateG}\n",
    "thresh ={\"noH\":noH, \"ovH\":ovH, \"obH\":obH, \"earlyH\":earlyH, \"midH\":midH, \"lateH\" :lateH}\n",
    "PARTS ={'noPART':noPART, \"ovPART\":ovPART, \"obPART\":obPART, \"earlyPART\":earlyPART, \"midPART\":midPART, \"latePART\":latePART}\n",
    "mean_cor={'mean_no':mean_no, 'mean_ov': mean_ov, 'mean_ob':mean_ob, 'mean_early':mean_early, 'mean_mid':mean_mid, 'mean_late':mean_late}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickling data to save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRAPHS['BIGdf'] = BIGdf\n",
    "# GRAPHS['unthresh'] = unthresh\n",
    "# GRAPHS['thresh'] = thresh\n",
    "# GRAPHS['partitions'] = PARTS\n",
    "# GRAPHS['labels'] = labels\n",
    "# GRAPHS['categories'] = categories\n",
    "# GRAPHS['mean_correlations'] = mean_cor\n",
    "# pickle.dump(GRAPHS, open('/Users/gracer/Google Drive/ABCD/tmp/4viz', 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baby = {}\n",
    "# baby['unthresh'] = unthresh\n",
    "# baby['thresh'] = thresh\n",
    "# baby['partitions'] = PARTS\n",
    "# baby['labels'] = labels\n",
    "# baby['categories'] = categories\n",
    "# baby['mean_correlations'] = mean_cor\n",
    "# pickle.dump(GRAPHS, open('/Users/gracer/Google Drive/ABCD/tmp/mini4viz', 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
