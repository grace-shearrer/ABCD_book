{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Analysis for ABCD data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://dx.plos.org/10.1371/journal.pbio.1002328  \n",
    "https://www.sciencedirect.com/science/article/pii/S105381191730109X?via%3Dihub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install python-louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gracer/.local/lib/python3.6/site-packages/vispy/visuals/isocurve.py:22: UserWarning: VisPy is not yet compatible with matplotlib 2.2+\n",
      "  warnings.warn(\"VisPy is not yet compatible with matplotlib 2.2+\")\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import community\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "import bz2\n",
    "import pickle\n",
    "import pdb\n",
    "import statistics\n",
    "import matplotlib\n",
    "matplotlib.use(\"Qt5Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from visbrain.objects import ConnectObj, SceneObj, SourceObj, BrainObj\n",
    "from visbrain.io import download_file\n",
    "\n",
    "import bct\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data of interest (from R notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('/Users/gracer/Google Drive/ABCD/important_txt/locations.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "interest =  pd.read_csv('/Users/gracer/Google Drive/ABCD/important_txt/data4analysis.txt', sep=\" \", header=None, \n",
    "                 index_col=False)\n",
    "interest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs=interest[0]\n",
    "subs[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = glob.glob('/Users/gracer/Google Drive/ABCD/ABCDworking/sub-*/keep/sub-NDAR*_ses-baselineYear1Arm1_task-rest_run-0*_bold_brain_norm_r_matrix.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a dictonary to store the path and subject ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_dict={}\n",
    "for item in data:\n",
    "    name=item.split('/')[6]\n",
    "    print(name)\n",
    "    my_dict.setdefault(name, []).append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a dictonary of all the file paths common to the subject list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dict = {k: my_dict[k] for k in subs if k in my_dict}\n",
    "print(my_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a dictonary to read in the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dict={}\n",
    "num=[]\n",
    "\n",
    "for key, value in path_dict.items():\n",
    "    num.append(len(value))\n",
    "    for i in value:\n",
    "        x=pd.read_csv(i, header=None,index_col=False)\n",
    "#         data_dict.setdefault(key, x)\n",
    "        data_dict.setdefault(key, []).append(pd.read_csv(i, header=None,index_col=False))\n",
    "#         data_dict[key]= pd.read_csv(i, header=None,index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(data_dict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to create dictionary of covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_dict=interest.set_index(interest[0]).to_dict()\n",
    "def removekey(d, key):\n",
    "    r = dict(d)\n",
    "    del r[key]\n",
    "    return r\n",
    "cov_dict=removekey(interest_dict, 0)\n",
    "cov_dict['sex'] = cov_dict.pop(1)\n",
    "cov_dict['PCS'] = cov_dict.pop(2)\n",
    "cov_dict['OVOB'] = cov_dict.pop(3)\n",
    "# cov_dict['BMItile'] = cov_dict.pop(4)\n",
    "# cov_dict['PDSscore'] = cov_dict.pop(5)\n",
    "# cov_dict['age'] = cov_dict.pop(6)\n",
    "print(cov_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to create graph objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corr_network_5(G, corr_direction, min_correlation):\n",
    "    ##Creates a copy of the graph\n",
    "    H = G.copy()\n",
    "    \n",
    "    ##Checks all the edges and removes some based on corr_direction\n",
    "    for stock1, stock2, weight in list(G.edges(data=True)):\n",
    "        ##if we only want to see the positive correlations we then delete the edges with weight smaller than 0        \n",
    "        if corr_direction == \"positive\":\n",
    "            ####it adds a minimum value for correlation. \n",
    "            ####If correlation weaker than the min, then it deletes the edge\n",
    "            if weight[\"weight\"] <0 or weight[\"weight\"] < min_correlation:\n",
    "                H.remove_edge(stock1, stock2)\n",
    "        ##this part runs if the corr_direction is negative and removes edges with weights equal or largen than 0\n",
    "        else:\n",
    "            ####it adds a minimum value for correlation. \n",
    "            ####If correlation weaker than the min, then it deletes the edge\n",
    "            if weight[\"weight\"] >=0 or weight[\"weight\"] > min_correlation:\n",
    "                H.remove_edge(stock1, stock2)\n",
    "    return(H)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to make a graph object BY SUBJECT\n",
    "This will return: \n",
    "* The edges (noramlized R correlation matrix, in pandas dataframe)\n",
    "* The mean_FC (the mean functional connectivity per subject/node)\n",
    "* The graphs (this will contain the raw graph object G as well as the the partion values from the modularity calculation)\n",
    "* The mu is the mean of all the runs into a single correlation matrix per subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graphs(list_o_data, direction, min_cor):\n",
    "    edge_dict={}\n",
    "    FC_dict={}\n",
    "    graph_dict={}\n",
    "    \n",
    "    print(len(list(list_o_data.keys())))\n",
    "    j=0\n",
    "    mylist=[]\n",
    "    mu_network={}\n",
    "    bad={}\n",
    "    for key, val_list in list_o_data.items():\n",
    "        print(\"on number %s\"%(str(j)))\n",
    "        j=j+1\n",
    "        newlist=[]\n",
    "        for item in val_list:\n",
    "            \n",
    "            print(np.array(item).diagonal())\n",
    "            if np.all(np.array(item).diagonal()) == True:\n",
    "                newlist.append(np.array(item))\n",
    "                i=item.set_index(labels.ID)\n",
    "                i.rename(columns=labels.ID, inplace=True)\n",
    "                edge_dict.setdefault(key, []).append(i)\n",
    "                \n",
    "            else:\n",
    "                print(\"%s is fucked\"%key)\n",
    "                bad.setdefault(key, []).append(item)\n",
    "                \n",
    "                \n",
    "        try:        \n",
    "            y=np.dstack(newlist)\n",
    "            print(y.shape)\n",
    "            y=np.rollaxis(y,-1)\n",
    "            print(y.shape)\n",
    "            mu=np.mean(y, axis=0)\n",
    "            print(np.array(mu).diagonal())\n",
    "            mu_network.setdefault(key, mu)\n",
    "\n",
    "            m=x.mean()\n",
    "            FC_dict.setdefault(key, []).append(m)\n",
    "\n",
    "            G = nx.from_numpy_matrix(mu)\n",
    "            for i, nlrow in labels.iterrows():\n",
    "                G.node[i].update(nlrow[0:].to_dict())\n",
    "\n",
    "            graph_dict.setdefault(key, []).append(G)\n",
    "\n",
    "            partition = community.best_partition(create_corr_network_5(G, direction,min_cor))\n",
    "\n",
    "            graph_dict.setdefault(key, []).append(partition)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "#         pdb.set_trace()\n",
    "              \n",
    "    return({'edges':edge_dict, 'correlations':cor_dict, 'mean_FC':FC_dict, 'graphs':graph_dict, 'mu':mu_network})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GRAPHS=make_graphs(data_dict, \"positive\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(GRAPHS['mu']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key in cov_dict.keys():\n",
    "    for subkey, value in cov_dict[key].items():\n",
    "        if subkey in GRAPHS['graphs']:\n",
    "            GRAPHS['graphs'][subkey].append(value)\n",
    "            print(GRAPHS['graphs'][subkey])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(list(GRAPHS['graphs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_dict['PCS']['sub-NDARINV019DXLU4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPHS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_two_dicts(x, y):\n",
    "    z = x.copy()   # start with x's keys and values\n",
    "    z.update(y)    # modifies z with y's keys and values & returns None\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = merge_two_dicts(GRAPHS, interest_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z['sex'] =z.pop(0)\n",
    "z['sex'] =z.pop(1)\n",
    "z['PCS'] =z.pop(2)\n",
    "z['OVOB'] =z.pop(3)\n",
    "# z['BMItile'] =z.pop(4)\n",
    "# z['PDSscore'] =z.pop(5)\n",
    "# z['age'] =z.pop(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to delete items in list that you accidentally made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key,value in GRAPHS['mean_FC'].items():\n",
    "#      del value[-7:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differences in modularity by subject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Participation coefficient \n",
    "#### Parameters\n",
    "    ----------\n",
    "    W : NxN np.ndarray\n",
    "        binary/weighted directed/undirected connection matrix\n",
    "    ci : Nx1 np.ndarray\n",
    "        community affiliation vector\n",
    "    degree : str\n",
    "        Flag to describe nature of graph 'undirected': For undirected graphs\n",
    "                                         'in': Uses the in-degree\n",
    "                                         'out': Uses the out-degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def participation_award(cor_mats, parts):\n",
    "#     cor_mats need to be something like a dictionary of correlation matrices with the subject as the key\n",
    "#     parts need to be the numerical modularity values\n",
    "    allPC={}\n",
    "    for keys, values in cor_mats.items():\n",
    "        print(keys)\n",
    "        cor_mat = np.array(values)\n",
    "        test_array=np.array(list(list(z['graphs'][keys])[1].values()))\n",
    "        testPART=np.vstack(test_array)\n",
    "\n",
    "        PC=bct.participation_coef(W=cor_mat, ci= testPART)\n",
    "        allPC[keys]=PC\n",
    "        \n",
    "    return(allPC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "allPC=participation_award(z['mu'],z['graphs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Coefficient \n",
    "The weighted clustering coefficient is the average \"intensity\" of\n",
    "    triangles around a node.\n",
    "   #### Parameters\n",
    "    ----------\n",
    "    W : NxN np.ndarray\n",
    "        weighted directed connection matrix\n",
    "    Returns\n",
    "    -------\n",
    "    C : Nx1 np.ndarray\n",
    "        clustering coefficient vector\n",
    "    Notes\n",
    "    -----\n",
    "    Methodological note (also see clustering_coef_bd)\n",
    "    The weighted modification is as follows:\n",
    "    - The numerator: adjacency matrix is replaced with weights matrix ^ 1/3\n",
    "    - The denominator: no changes from the binary version\n",
    "    The above reduces to symmetric and/or binary versions of the clustering\n",
    "    coefficient for respective graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_fuq(cor_mats):\n",
    "    clusters={}\n",
    "    for keys, values in cor_mats.items():\n",
    "        CC=bct.clustering_coef_wd(values)\n",
    "        clusters[keys]=CC\n",
    "    return(clusters)\n",
    "#         pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allCC=cluster_fuq(z['mu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UR SO Random\n",
    "def null_model_und_sign(W, bin_swaps=5, wei_freq=.1, seed=None):\n",
    "    '''\n",
    "    This function randomizes an undirected network with positive and\n",
    "    negative weights, while preserving the degree and strength\n",
    "    distributions. This function calls randmio_und.m\n",
    "#### Parameters\n",
    "    ----------\n",
    "    W : NxN np.ndarray\n",
    "        undirected weighted connection matrix\n",
    "    bin_swaps : int\n",
    "        average number of swaps in each edge binary randomization. Default\n",
    "        value is 5. 0 swaps implies no binary randomization.\n",
    "    wei_freq : float\n",
    "        frequency of weight sorting in weighted randomization. 0<=wei_freq<1.\n",
    "        wei_freq == 1 implies that weights are sorted at each step.\n",
    "        wei_freq == 0.1 implies that weights sorted each 10th step (faster,\n",
    "            default value)\n",
    "        wei_freq == 0 implies no sorting of weights (not recommended)\n",
    "    seed : hashable, optional\n",
    "        If None (default), use the np.random's global random state to generate random numbers.\n",
    "        Otherwise, use a new np.random.RandomState instance seeded with the given value.\n",
    "    Returns\n",
    "    -------\n",
    "    W0 : NxN np.ndarray\n",
    "        randomized weighted connection matrix\n",
    "    R : 4-tuple of floats\n",
    "        Correlation coefficients between strength sequences of input and\n",
    "        output connection matrices, rpos_in, rpos_out, rneg_in, rneg_out\n",
    "    Notes\n",
    "    -----\n",
    "    The value of bin_swaps is ignored when binary topology is fully\n",
    "        connected (e.g. when the network has no negative weights).\n",
    "    Randomization may be better (and execution time will be slower) for\n",
    "        higher values of bin_swaps and wei_freq. Higher values of bin_swaps\n",
    "        may enable a more random binary organization, and higher values of\n",
    "        wei_freq may enable a more accurate conservation of strength\n",
    "        sequences.\n",
    "    R are the correlation coefficients between positive and negative\n",
    "        strength sequences of input and output connection matrices and are\n",
    "        used to evaluate the accuracy with which strengths were preserved.\n",
    "        Note that correlation coefficients may be a rough measure of\n",
    "        strength-sequence accuracy and one could implement more formal tests\n",
    "        (such as the Kolmogorov-Smirnov test) if desired.\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UR_SO_RANDOM(cor_mat):\n",
    "    nullz={}\n",
    "    for keys, values in cor_mat.items():\n",
    "        print(keys)\n",
    "        test=bct.null_model_und_sign(values, bin_swaps=5, wei_freq=.1)\n",
    "        pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UR_SO_RANDOM(z['mu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_model_und_sign(W, bin_swaps=5, wei_freq=.1, seed=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized information score  \n",
    "sklearn.metrics.normalized_mutual_info_score(labels_true [group 1], labels_pred [group 2], average_method=’warn’)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_no_ov=normalized_mutual_info_score(norm_max, ov_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_no_ob=normalized_mutual_info_score(norm_max, ob_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_ov_ob=normalized_mutual_info_score(ov_max, ob_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parcelation \n",
    "Through BIAC https://wiki.biac.duke.edu/biac:analysis:resting_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual and Group Matrices\n",
    "Network-level analysis will be performed with inividual correltion matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thresholding\n",
    "In accordance with van den Heuvel et al. 2017, we will examine and test statistical differences in functional connectivity (FC) defined as the mean of the correlation matrix. FC will be included in statistical tests between groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning\n",
    "Will partition full 264 connectome into modules using louvain algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the partition\n",
    "Will use normalized mutual information to assess similarity between network assignments. NMI measures information shared between two probability distribution functions, specifically measuring how much knowing one distribution leads to certainty ofthe other. Permuted the labels of individual matrices between contrasts 1,000 times to generate a null distribution of NMI values for each contrast. Matrices between groups were randomly shuffled and partitioned into functional networks, and NMI was calculated.   \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.normalized_mutual_info_score.html#sklearn.metrics.normalized_mutual_info_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connectivity Strength\n",
    "Caluclate Euclidean distance for each ROI-ROI pair. Linear regression with distance as a predictor of connectivity strength between groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Within network changes\n",
    "All within network pairwise relationships were averaged per group. Two-tailed T-test to assess differences. Bonferroni corrections as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Between network changes\n",
    "Average connectivity is calculated per network. Compare the between network interactions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Participation Coefficient\n",
    "Partition networks into the modules, calculate the PC per node within each group. Higher PC indicates more distributed between network connectivity, while a PC of 0 signifies a node’s links are completely within its home network (within network)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickling data to save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sfile = bz2.BZ2File('/Users/gracer/Google Drive/ABCD/tmp/smallerfile', 'w')\n",
    "# pickle.dump(GRAPHS, sfile)\n",
    "# z['allCC'] = allCC\n",
    "# z['allPC'] = allPC\n",
    "# pickle.dump(z, open('/Users/gracer/Google Drive/ABCD/tmp/current', 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
